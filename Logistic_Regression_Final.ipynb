{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train  = pd.read_csv('emnist-letters-train.csv')\n",
    "data_test = pd.read_csv('emnist-letters-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[23]\n",
      " [ 7]\n",
      " [16]\n",
      " ...\n",
      " [ 1]\n",
      " [23]\n",
      " [12]]\n"
     ]
    }
   ],
   "source": [
    "X = data_train.drop('Y',axis = 1).values\n",
    "X = X.T\n",
    "X = X/255.0\n",
    "y = data_train['Y'].values.reshape(data_train.shape[0],1)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " ...\n",
      " [19]\n",
      " [19]\n",
      " [19]]\n"
     ]
    }
   ],
   "source": [
    "X_test = data_test.drop('Y',axis = 1).values\n",
    "X_test = X_test.T\n",
    "X_test = X_test/255.0\n",
    "y_test = data_test['Y'].values.reshape(data_test.shape[0],1)\n",
    "print(y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = []\n",
    "for i in range(26):\n",
    "  t = []\n",
    "  for j in range(14800):\n",
    "      if y_test[j] == (i+1):\n",
    "          t.append(1)\n",
    "      else:\n",
    "          t.append(0)\n",
    "  Y_test.append(t)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = []\n",
    "for i in range(26):\n",
    "  t = []\n",
    "  for j in range(88800):\n",
    "      if y[j] == (i+1):\n",
    "          t.append(1)\n",
    "      else:\n",
    "          t.append(0)\n",
    "  Y.append(t)      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 6000\n",
    "alpha = 1\n",
    "m = 88800\n",
    "cost_values = []\n",
    "trained_parameters = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for datset0\n",
      "running @ 3.476798178135336\n",
      "running @ 0.15230806260790852\n",
      "running @ 0.1281831095053652\n",
      "running @ 0.11894099967585013\n",
      "running @ 0.11411025099342241\n",
      "running @ 0.1110675031334823\n",
      "running @ 0.1089109491371224\n",
      "running @ 0.1072636706022376\n",
      "running @ 0.10594383722684604\n",
      "running @ 0.10485308657243017\n",
      "running @ 0.10393275450993515\n",
      "running @ 0.10314480194631734\n",
      "running @ 0.10246280953749135\n",
      "running @ 0.10186742455366143\n",
      "running @ 0.10134390232207795\n",
      "running @ 0.10088068727203209\n",
      "running @ 0.10046853799990058\n",
      "running @ 0.10009995351404169\n",
      "Training for datset1\n",
      "running @ 3.5429647007295246\n",
      "running @ 0.14373596762144344\n",
      "running @ 0.11225750276297664\n",
      "running @ 0.10138037768918587\n",
      "running @ 0.09592520317416268\n",
      "running @ 0.09257896015091353\n",
      "running @ 0.09026786033556919\n",
      "running @ 0.0885485638816507\n",
      "running @ 0.0872033512824918\n",
      "running @ 0.08611206054637041\n",
      "running @ 0.08520285416393016\n",
      "running @ 0.0844300508823465\n",
      "running @ 0.083763044233297\n",
      "running @ 0.08318037995226962\n",
      "running @ 0.08266642660767676\n",
      "running @ 0.08220941928514754\n",
      "running @ 0.0818002621433176\n",
      "running @ 0.08143176691269641\n",
      "Training for datset2\n",
      "running @ 0.7227728346018695\n",
      "running @ 0.08545152195533334\n",
      "running @ 0.07290622558594884\n",
      "running @ 0.06697510921579806\n",
      "running @ 0.06344141990765756\n",
      "running @ 0.061095423717730815\n",
      "running @ 0.05941657206328027\n",
      "running @ 0.058147588129596764\n",
      "running @ 0.05714881237549158\n",
      "running @ 0.05633828528696442\n",
      "running @ 0.055664738907438914\n",
      "running @ 0.05509441155358841\n",
      "running @ 0.05460411744151614\n",
      "running @ 0.05417736465436622\n",
      "running @ 0.05380206474124153\n",
      "running @ 0.05346912361700232\n",
      "running @ 0.053171544080814745\n",
      "running @ 0.05290383668258903\n",
      "Training for datset3\n",
      "running @ 1.1141188958396067\n",
      "running @ 0.15507436743506087\n",
      "running @ 0.11950642214474265\n",
      "running @ 0.10682476156571191\n",
      "running @ 0.1009144092508065\n",
      "running @ 0.09744545075193958\n",
      "running @ 0.09509771932542652\n",
      "running @ 0.09336971241167806\n",
      "running @ 0.09202845909039974\n",
      "running @ 0.09094895826366367\n",
      "running @ 0.0900571092418495\n",
      "running @ 0.08930567000711748\n",
      "running @ 0.0886627805932569\n",
      "running @ 0.08810597487510362\n",
      "running @ 0.0876188344093681\n",
      "running @ 0.08718900853902067\n",
      "running @ 0.08680698404628451\n",
      "running @ 0.08646528787574123\n",
      "Training for datset4\n",
      "running @ inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_11696\\4141458307.py:11: RuntimeWarning: divide by zero encountered in log\n",
      "  j = 1/m*(-1*(np.sum(Z*np.log(hy)+(1-Z)*np.log(1-hy))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running @ 0.11320780233135443\n",
      "running @ 0.09311893652399295\n",
      "running @ 0.08523683175425462\n",
      "running @ 0.0808798992388655\n",
      "running @ 0.0781407449844315\n",
      "running @ 0.07627569806079318\n",
      "running @ 0.0749267586961485\n",
      "running @ 0.07390391268375245\n",
      "running @ 0.07309909229045758\n",
      "running @ 0.07244709881315776\n",
      "running @ 0.07190651924842972\n",
      "running @ 0.0714498452803855\n",
      "running @ 0.07105808524655513\n",
      "running @ 0.07071768530908253\n",
      "running @ 0.07041869390176943\n",
      "running @ 0.07015362465350464\n",
      "running @ 0.06991672725337719\n",
      "Training for datset5\n",
      "running @ 1.3293941004074283\n",
      "running @ 0.10158058925722933\n",
      "running @ 0.0842121602249453\n",
      "running @ 0.07745126090999022\n",
      "running @ 0.07373630610835087\n",
      "running @ 0.07135689636581509\n",
      "running @ 0.06968231193152123\n",
      "running @ 0.06842588382646855\n",
      "running @ 0.0674389296028296\n",
      "running @ 0.06663682174048184\n",
      "running @ 0.06596796663324458\n",
      "running @ 0.06539914647293534\n",
      "running @ 0.06490795632383542\n",
      "running @ 0.06447864719756792\n",
      "running @ 0.06409973421027709\n",
      "running @ 0.06376256943516011\n",
      "running @ 0.06346046195405275\n",
      "running @ 0.06318811750486274\n",
      "Training for datset6\n",
      "running @ 1.6382924539541948\n",
      "running @ 0.16373985473547256\n",
      "running @ 0.13241421291831196\n",
      "running @ 0.12227693835201474\n",
      "running @ 0.1173648535246999\n",
      "running @ 0.11439900809265278\n",
      "running @ 0.11234819576932689\n",
      "running @ 0.11080177737968737\n",
      "running @ 0.10956942622059324\n",
      "running @ 0.10855181150478774\n",
      "running @ 0.10769147473344295\n",
      "running @ 0.1069520833862147\n",
      "running @ 0.10630891906378959\n",
      "running @ 0.10574418210029377\n",
      "running @ 0.10524450062184777\n",
      "running @ 0.10479951181830621\n",
      "running @ 0.1044009975840484\n",
      "running @ 0.1040423252872755\n",
      "Training for datset7\n",
      "running @ inf\n",
      "running @ 0.11873369273781294\n",
      "running @ 0.10299718259400066\n",
      "running @ 0.09606614456154276\n",
      "running @ 0.09192059914876916\n",
      "running @ 0.0890904882118922\n",
      "running @ 0.08700962051159397\n",
      "running @ 0.08540563067769168\n",
      "running @ 0.08412809652984943\n",
      "running @ 0.08308573682018511\n",
      "running @ 0.08221928196329074\n",
      "running @ 0.08148823768221115\n",
      "running @ 0.08086381351199527\n",
      "running @ 0.08032487833365573\n",
      "running @ 0.07985551943385755\n",
      "running @ 0.07944350123680466\n",
      "running @ 0.0790792529956071\n",
      "running @ 0.07875518058155388\n",
      "Training for datset8\n",
      "running @ inf\n",
      "running @ 0.11147714168734152\n",
      "running @ 0.0966851921213726\n",
      "running @ 0.08937645333016232\n",
      "running @ 0.08505665806619382\n",
      "running @ 0.08229275641942235\n",
      "running @ 0.08041489946214707\n",
      "running @ 0.0790673756324664\n",
      "running @ 0.07805140843632156\n",
      "running @ 0.07725259678347386\n",
      "running @ 0.07660275989927688\n",
      "running @ 0.07605955972823311\n",
      "running @ 0.07559558958335096\n",
      "running @ 0.0751924069794229\n",
      "running @ 0.07483715892910744\n",
      "running @ 0.07452060234598347\n",
      "running @ 0.07423590150570293\n",
      "running @ 0.07397787363811847\n",
      "Training for datset9\n",
      "running @ inf\n",
      "running @ 0.09594195658051821\n",
      "running @ 0.07966466558757074\n",
      "running @ 0.0734511536998605\n",
      "running @ 0.06991587321088566\n",
      "running @ 0.06756243608623934\n",
      "running @ 0.06585932082394597\n",
      "running @ 0.06455799639295133\n",
      "running @ 0.06352487205248387\n",
      "running @ 0.06268117256814848\n",
      "running @ 0.06197704993457776\n",
      "running @ 0.0613791778181827\n",
      "running @ 0.06086429928542534\n",
      "running @ 0.060415624995743725\n",
      "running @ 0.06002069752849298\n",
      "running @ 0.059670060653608595\n",
      "running @ 0.059356396196693896\n",
      "running @ 0.05907394549184534\n",
      "Training for datset10\n",
      "running @ 2.6679211985961713\n",
      "running @ 0.12771556299538117\n",
      "running @ 0.10745937980242927\n",
      "running @ 0.09911534008438314\n",
      "running @ 0.09454149002863443\n",
      "running @ 0.0916049355348229\n",
      "running @ 0.08951968178213551\n",
      "running @ 0.08793831755318851\n",
      "running @ 0.086684435374668\n",
      "running @ 0.08565859721900373\n",
      "running @ 0.08479998547243242\n",
      "running @ 0.08406887454774079\n",
      "running @ 0.08343789557198408\n",
      "running @ 0.08288736668167168\n",
      "running @ 0.08240264107227117\n",
      "running @ 0.08197251786009163\n",
      "running @ 0.08158824298184403\n",
      "running @ 0.08124285365659138\n",
      "Training for datset11\n",
      "running @ 6.554857887635643\n",
      "running @ 0.10809480142974555\n",
      "running @ 0.09353531386213162\n",
      "running @ 0.08758844771140817\n",
      "running @ 0.0840840140177601\n",
      "running @ 0.0817019076600019\n",
      "running @ 0.07995942566306617\n",
      "running @ 0.07862643260453539\n",
      "running @ 0.07757467154354011\n",
      "running @ 0.07672533848142028\n",
      "running @ 0.0760266923772522\n",
      "running @ 0.07544309515269293\n",
      "running @ 0.07494910539238335\n",
      "running @ 0.074526053995173\n",
      "running @ 0.07415994102041606\n",
      "running @ 0.07384008273199132\n",
      "running @ 0.07355820759235913\n",
      "running @ 0.07330783290419587\n",
      "Training for datset12\n",
      "running @ 4.31472801951535\n",
      "running @ 0.07722731073488308\n",
      "running @ 0.0617763898974635\n",
      "running @ 0.05405489191837229\n",
      "running @ 0.04940263898944366\n",
      "running @ 0.04628785956302252\n",
      "running @ 0.04405358055306297\n",
      "running @ 0.04236867498532164\n",
      "running @ 0.04104975564345863\n",
      "running @ 0.03998759725156222\n",
      "running @ 0.039113004113010316\n",
      "running @ 0.038379833463700046\n",
      "running @ 0.037756001722138985\n",
      "running @ 0.037218435704522164\n",
      "running @ 0.03675008592278988\n",
      "running @ 0.03633807424420381\n",
      "running @ 0.03597249575242018\n",
      "running @ 0.035645614989994266\n",
      "Training for datset13\n",
      "running @ 3.0113675312033528\n",
      "running @ 0.13083499726018483\n",
      "running @ 0.10860396029336883\n",
      "running @ 0.09996133447865382\n",
      "running @ 0.09527520634767575\n",
      "running @ 0.09226634111725786\n",
      "running @ 0.09014035096584799\n",
      "running @ 0.08854174114020177\n",
      "running @ 0.08728556067331197\n",
      "running @ 0.08626556178901539\n",
      "running @ 0.08541629229424984\n",
      "running @ 0.0846952508905616\n",
      "running @ 0.08407361976102483\n",
      "running @ 0.0835311072739386\n",
      "running @ 0.08305293722105178\n",
      "running @ 0.08262802551091124\n",
      "running @ 0.08224784018564354\n",
      "running @ 0.08190566696629116\n",
      "Training for datset14\n",
      "running @ inf\n",
      "running @ 0.08779643323113967\n",
      "running @ 0.06956151716516931\n",
      "running @ 0.06265004707829891\n",
      "running @ 0.05910731348275624\n",
      "running @ 0.05700741071355869\n",
      "running @ 0.05563578812563059\n",
      "running @ 0.05467393378529457\n",
      "running @ 0.05396134739265904\n",
      "running @ 0.053409771033567026\n",
      "running @ 0.052967384019311885\n",
      "running @ 0.05260213253094303\n",
      "running @ 0.05229331572236186\n",
      "running @ 0.05202706509627402\n",
      "running @ 0.05179378900338126\n",
      "running @ 0.051586665008715456\n",
      "running @ 0.051400717329277035\n",
      "running @ 0.051232234090751116\n",
      "Training for datset15\n",
      "running @ 4.88606342857839\n",
      "running @ 0.088495347703873\n",
      "running @ 0.07391929101175317\n",
      "running @ 0.06712868678478018\n",
      "running @ 0.06316047848606106\n",
      "running @ 0.060562155425886756\n",
      "running @ 0.05872886724486669\n",
      "running @ 0.057363863512677186\n",
      "running @ 0.0563059944802788\n",
      "running @ 0.055460482358243086\n",
      "running @ 0.054767858585535595\n",
      "running @ 0.054188844098612535\n",
      "running @ 0.053696438971237645\n",
      "running @ 0.05327149792648853\n",
      "running @ 0.05290010320629405\n",
      "running @ 0.052571924859197205\n",
      "running @ 0.05227915608875023\n",
      "running @ 0.05201580040298504\n",
      "Training for datset16\n",
      "running @ 0.8415370695792759\n",
      "running @ 0.1512174511576791\n",
      "running @ 0.11708808846607557\n",
      "running @ 0.10659949709261353\n",
      "running @ 0.1018346288936841\n",
      "running @ 0.09909804468355904\n",
      "running @ 0.09728877691086525\n",
      "running @ 0.09597610709041736\n",
      "running @ 0.09496087637225663\n",
      "running @ 0.09413992476098258\n",
      "running @ 0.0934549192909588\n",
      "running @ 0.09287036408667505\n",
      "running @ 0.09236324222489466\n",
      "running @ 0.09191778259998046\n",
      "running @ 0.09152265969881693\n",
      "running @ 0.09116941761841059\n",
      "running @ 0.09085154061444936\n",
      "running @ 0.09056387995438683\n",
      "Training for datset17\n",
      "running @ 2.3696711019648884\n",
      "running @ 0.12434099272594493\n",
      "running @ 0.10540213156579176\n",
      "running @ 0.09848860324544759\n",
      "running @ 0.09474482162936958\n",
      "running @ 0.09233490994869405\n",
      "running @ 0.09062484709213012\n",
      "running @ 0.08933155093392008\n",
      "running @ 0.08830844925209584\n",
      "running @ 0.08747190794029834\n",
      "running @ 0.08677069325159881\n",
      "running @ 0.08617159339832055\n",
      "running @ 0.0856520331828535\n",
      "running @ 0.08519603141805594\n",
      "running @ 0.08479187472482315\n",
      "running @ 0.08443072115172096\n",
      "running @ 0.08410572945170339\n",
      "running @ 0.08381149654276741\n",
      "Training for datset18\n",
      "running @ 2.1520951950006397\n",
      "running @ 0.0821524893842567\n",
      "running @ 0.06616563760696544\n",
      "running @ 0.059985868533305106\n",
      "running @ 0.05667747657573465\n",
      "running @ 0.054591197663307174\n",
      "running @ 0.05313520514934241\n",
      "running @ 0.052046466232989454\n",
      "running @ 0.05119077586340848\n",
      "running @ 0.05049270966946589\n",
      "running @ 0.04990675767206982\n",
      "running @ 0.04940391333993103\n",
      "running @ 0.04896484846476031\n",
      "running @ 0.048576189859176395\n",
      "running @ 0.048228375275647876\n",
      "running @ 0.04791436384391866\n",
      "running @ 0.047628832142649645\n",
      "running @ 0.04736765765280016\n",
      "Training for datset19\n",
      "running @ 0.6097845323858283\n",
      "running @ 0.1474537265020847\n",
      "running @ 0.12778079308371906\n",
      "running @ 0.11978288693145721\n",
      "running @ 0.11525607540799669\n",
      "running @ 0.11225548588372974\n",
      "running @ 0.11007603437939331\n",
      "running @ 0.10839763849667267\n",
      "running @ 0.10705387062853992\n",
      "running @ 0.10594846704098944\n",
      "running @ 0.1050205835947218\n",
      "running @ 0.10422914945337927\n",
      "running @ 0.10354510904293211\n",
      "running @ 0.1029471989068432\n",
      "running @ 0.10241946025779029\n",
      "running @ 0.10194967769738934\n",
      "running @ 0.10152834917772631\n",
      "running @ 0.10114798001744607\n",
      "Training for datset20\n",
      "running @ inf\n",
      "running @ 0.12531819576388475\n",
      "running @ 0.10215905676371208\n",
      "running @ 0.0931209276868251\n",
      "running @ 0.08851474389211678\n",
      "running @ 0.0857407912548375\n",
      "running @ 0.08384271332600218\n",
      "running @ 0.08242270106608501\n",
      "running @ 0.08129570661737885\n",
      "running @ 0.08036622025393712\n",
      "running @ 0.07957997961571778\n",
      "running @ 0.07890334479638719\n",
      "running @ 0.0783138224568935\n",
      "running @ 0.07779541167557312\n",
      "running @ 0.07733616658778823\n",
      "running @ 0.07692683764800985\n",
      "running @ 0.07656006686808244\n",
      "running @ 0.07622988285338653\n",
      "Training for datset21\n",
      "running @ 0.7502838745499357\n",
      "running @ 0.09579729629014551\n",
      "running @ 0.08447662639015499\n",
      "running @ 0.0794663111785933\n",
      "running @ 0.07638387081343649\n",
      "running @ 0.07424707392324646\n",
      "running @ 0.0726630084667794\n",
      "running @ 0.07143243367406622\n",
      "running @ 0.07044209772337152\n",
      "running @ 0.06962309835065165\n",
      "running @ 0.06893128152183495\n",
      "running @ 0.06833704878029294\n",
      "running @ 0.06781975247784003\n",
      "running @ 0.06736448111338961\n",
      "running @ 0.0669601418487716\n",
      "running @ 0.06659827247464518\n",
      "running @ 0.06627227840653875\n",
      "running @ 0.06597692622573854\n",
      "Training for datset22\n",
      "running @ 2.1683929826525064\n",
      "running @ 0.08664170853639457\n",
      "running @ 0.06889604232791892\n",
      "running @ 0.06055932294277321\n",
      "running @ 0.055656525449030804\n",
      "running @ 0.05241886035641562\n",
      "running @ 0.05010639667234414\n",
      "running @ 0.048365327687688475\n",
      "running @ 0.04700484809851376\n",
      "running @ 0.045911609579399215\n",
      "running @ 0.04501346815207578\n",
      "running @ 0.044262137724777\n",
      "running @ 0.043624042794345515\n",
      "running @ 0.043075120287396344\n",
      "running @ 0.04259768529058702\n",
      "running @ 0.04217845475984676\n",
      "running @ 0.041807258651888564\n",
      "running @ 0.04147617647636661\n",
      "Training for datset23\n",
      "running @ 1.0913915910314425\n",
      "running @ 0.11884797109285841\n",
      "running @ 0.09738521851421685\n",
      "running @ 0.0879110795922927\n",
      "running @ 0.0826026480655169\n",
      "running @ 0.07923976076516874\n",
      "running @ 0.07693551383560841\n",
      "running @ 0.07526270290530618\n",
      "running @ 0.07399125596706886\n",
      "running @ 0.0729879696847012\n",
      "running @ 0.07217153658480492\n",
      "running @ 0.07149015773801415\n",
      "running @ 0.07090960066836191\n",
      "running @ 0.07040649732791598\n",
      "running @ 0.0699644257128085\n",
      "running @ 0.06957153728286833\n",
      "running @ 0.06921907506151835\n",
      "running @ 0.06890042181469982\n",
      "Training for datset24\n",
      "running @ inf\n",
      "running @ 0.11097734778113963\n",
      "running @ 0.09488397419702889\n",
      "running @ 0.08711445015918524\n",
      "running @ 0.08225259892247598\n",
      "running @ 0.07895280031342747\n",
      "running @ 0.07659793941979998\n",
      "running @ 0.07484686919243162\n",
      "running @ 0.07349675305007951\n",
      "running @ 0.07242252916901755\n",
      "running @ 0.07154495455987009\n",
      "running @ 0.07081231656082539\n",
      "running @ 0.07018976220825622\n",
      "running @ 0.06965302232868052\n",
      "running @ 0.06918466922038256\n",
      "running @ 0.0687718366452504\n",
      "running @ 0.06840479259764538\n",
      "running @ 0.06807601924644352\n",
      "Training for datset25\n",
      "running @ 2.27046848325947\n",
      "running @ 0.11539029560477396\n",
      "running @ 0.0909522439991848\n",
      "running @ 0.08137738413115585\n",
      "running @ 0.07634697871872534\n",
      "running @ 0.07326625841863425\n",
      "running @ 0.07118252961381638\n",
      "running @ 0.0696743316624725\n",
      "running @ 0.06852715124966004\n",
      "running @ 0.0676203207150169\n",
      "running @ 0.06688106843064959\n",
      "running @ 0.06626319413908333\n",
      "running @ 0.0657361462312852\n",
      "running @ 0.06527901703907583\n",
      "running @ 0.06487706336313295\n",
      "running @ 0.06451960563403354\n",
      "running @ 0.064198716289104\n",
      "running @ 0.06390837745084713\n"
     ]
    }
   ],
   "source": [
    "for tr in range (26):\n",
    "    U = Y[tr]\n",
    "    Z = np.array(U)\n",
    "    weights = np.random.randn(1,784)\n",
    "    bias = 0\n",
    "    costfunc_values = []\n",
    "    print('Training for datset' +str(tr))\n",
    "    for i in range(0,iterations+1):\n",
    "       z = np.dot(weights,X)+bias\n",
    "       hy = 1/(1+np.exp(-z))\n",
    "       j = 1/m*(-1*(np.sum(Z*np.log(hy)+(1-Z)*np.log(1-hy))))\n",
    "       costfunc_values.append(j)\n",
    "       dw = 1/m*np.dot(hy-Z,X.T)\n",
    "       db = 1/m*np.sum(hy-Z)\n",
    "       weights = weights -alpha*dw\n",
    "       bias = bias - alpha*db\n",
    "       if(i%350) == 0:\n",
    "           print('running @',j)\n",
    "    cost_values.append(costfunc_values)\n",
    "    trained_parameters.append([weights,bias])\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for dataset0 = 0.9456644144144144\n",
      "accuracy for dataset1 = 0.9383333333333334\n",
      "accuracy for dataset2 = 0.9281756756756757\n",
      "accuracy for dataset3 = 0.9405405405405406\n",
      "accuracy for dataset4 = 0.9344594594594594\n",
      "accuracy for dataset5 = 0.932240990990991\n",
      "accuracy for dataset6 = 0.9474099099099099\n",
      "accuracy for dataset7 = 0.9358671171171171\n",
      "accuracy for dataset8 = 0.9327815315315315\n",
      "accuracy for dataset9 = 0.9304504504504505\n",
      "accuracy for dataset10 = 0.937240990990991\n",
      "accuracy for dataset11 = 0.9350900900900901\n",
      "accuracy for dataset12 = 0.9261936936936936\n",
      "accuracy for dataset13 = 0.9378716216216216\n",
      "accuracy for dataset14 = 0.9290315315315315\n",
      "accuracy for dataset15 = 0.9280067567567568\n",
      "accuracy for dataset16 = 0.9411261261261261\n",
      "accuracy for dataset17 = 0.9381081081081081\n",
      "accuracy for dataset18 = 0.9297635135135135\n",
      "accuracy for dataset19 = 0.9470495495495496\n",
      "accuracy for dataset20 = 0.935045045045045\n",
      "accuracy for dataset21 = 0.9319256756756756\n",
      "accuracy for dataset22 = 0.9268130630630631\n",
      "accuracy for dataset23 = 0.935168918918919\n",
      "accuracy for dataset24 = 0.9314076576576577\n",
      "accuracy for dataset25 = 0.9838288288288288\n"
     ]
    }
   ],
   "source": [
    "for i in range (26):\n",
    "    U = Y[tr]\n",
    "    Z = np.array(U)\n",
    "    weights = trained_parameters[i][0]\n",
    "    bias = trained_parameters[i][1]\n",
    "    correct_predictions = 0\n",
    "    for j in range(88800):\n",
    "        z = np.dot(weights,X.T[j])+bias\n",
    "        hypothesis = 1/(1+np.exp(-z))\n",
    "        if np.logical_and(hypothesis >= 0.5,Z[j] == 1):\n",
    "            correct_predictions+=1\n",
    "        if np.logical_and(hypothesis <0.5 , Z[j] == 0):\n",
    "            correct_predictions+=1\n",
    "    acc = (correct_predictions/88800)\n",
    "    print('accuracy for dataset' + str(i), \"=\" , acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5  1  1 ... 19 10  3]\n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "a = 0\n",
    "for i in range (14800):\n",
    "    probabilities = []\n",
    "    for j in range(19):\n",
    "        weights = trained_parameters[j][0]\n",
    "        bias = trained_parameters[j][1]\n",
    "        z = np.dot(weights,X_test.T[i]) +bias\n",
    "        hypothesis = 1/(1+np.exp(-z))\n",
    "        probabilities.append(hypothesis)\n",
    "        \n",
    "    predict = probabilities.index(max(probabilities))\n",
    "    predict += 1\n",
    "    y_pred.append(predict)\n",
    "    \n",
    "y_pred = np.array(y_pred)\n",
    "y_pred = y_pred.T\n",
    "print(y_pred)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.7046621621621622\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for i in range(14800):\n",
    " if (y_pred[i] == y_test[i]):\n",
    "  c +=1 \n",
    "print(\"accuracy = \", c/14800)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a29e1fb6683002b636560db254076994f59567f0270ab4cc0d58f0bc21ebc82f"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
